{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a1e0257-0021-47cf-9f16-420681d70021",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T11:09:18.411878Z",
     "iopub.status.busy": "2024-09-11T11:09:18.411878Z",
     "iopub.status.idle": "2024-09-11T11:09:20.743597Z",
     "shell.execute_reply": "2024-09-11T11:09:20.743502Z",
     "shell.execute_reply.started": "2024-09-11T11:09:18.411878Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directory: C:\\Users\\MSI\\Documents\\Migration Tool New\\DB Tables\\Contacts DB\\AWS Code\n",
      "Uploaded 20220830_220049.jpg to S3 bucket mydaybucketlive in folder live/Image/.\n",
      "Uploaded Statement #4 - OWN02487 .pdf to S3 bucket mydaybucketlive in folder live/Image/.\n",
      "Upload complete. Data saved to C:\\Users\\MSI\\Documents\\Migration Tool New\\DB Tables\\Contacts DB\\AWS Code\\s3_uploaded_files.xlsx.\n"
     ]
    }
   ],
   "source": [
    "#jpg and pdf good\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# AWS Credentials\n",
    "AWS_ACCESS_KEY_ID = \"AKIAQ4WCASZZHAHYXDDA\"\n",
    "AWS_SECRET_ACCESS_KEY = \"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\"\n",
    "AWS_DEFAULT_REGION = \"ap-southeast-2\"\n",
    "AWS_BUCKET = \"mydaybucketlive\"\n",
    "\n",
    "# S3 folder name\n",
    "S3_FOLDER = \"live/Image/\"\n",
    "\n",
    "# Directory containing the .jpg and .pdf files\n",
    "directory_path = r\"C:\\Users\\MSI\\Documents\\Migration Tool New\\DB Tables\\Contacts DB\\AWS Code\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_DEFAULT_REGION\n",
    ")\n",
    "\n",
    "# List to store file names and their corresponding URLs\n",
    "data = []\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"Directory {directory_path} does not exist.\")\n",
    "else:\n",
    "    print(f\"Scanning directory: {directory_path}\")\n",
    "\n",
    "    # Upload files and generate object URLs\n",
    "    files_found = False\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Process only .jpg and .pdf files\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".pdf\"):\n",
    "            files_found = True\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            try:\n",
    "                # Define S3 key (path) for the file inside the 'live/Image/' folder\n",
    "                s3_key = f\"{S3_FOLDER}{filename}\"\n",
    "\n",
    "                # Upload file to S3 under the 'live/Image/' folder\n",
    "                s3_client.upload_file(file_path, AWS_BUCKET, s3_key)\n",
    "                print(f\"Uploaded {filename} to S3 bucket {AWS_BUCKET} in folder {S3_FOLDER}.\")\n",
    "\n",
    "                # Generate the object URL\n",
    "                object_url = f\"https://{AWS_BUCKET}.s3.{AWS_DEFAULT_REGION}.amazonaws.com/{s3_key}\"\n",
    "\n",
    "                # Append file name and object URL to the data list\n",
    "                data.append({'File Name': filename, 'Object URL': object_url})\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {filename}: {str(e)}\")\n",
    "\n",
    "    if not files_found:\n",
    "        print(\"No .jpg or .pdf files found in the directory.\")\n",
    "\n",
    "    # If data is not empty, save to Excel\n",
    "    if data:\n",
    "        # Convert data to a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Define output Excel file path\n",
    "        output_excel = os.path.join(directory_path, 's3_uploaded_files.xlsx')\n",
    "\n",
    "        # Save to Excel\n",
    "        df.to_excel(output_excel, index=False)\n",
    "        print(f\"Upload complete. Data saved to {output_excel}.\")\n",
    "    else:\n",
    "        print(\"No files uploaded, Excel file was not generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ed4367-bbf3-4b82-a6c7-39f8fc1d5243",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45fd529c-0315-4d69-97c9-0815e775ae22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e811c464-df73-4243-b6cf-d1c532e6dac1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc6c2cd-d24d-4ac0-b79c-cf1bf9d2843b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "96e0bd98-5331-41d8-9e3d-b8f39fcf23da",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T11:27:57.149736Z",
     "iopub.status.busy": "2024-09-11T11:27:57.148732Z",
     "iopub.status.idle": "2024-09-11T11:28:22.148069Z",
     "shell.execute_reply": "2024-09-11T11:28:22.148069Z",
     "shell.execute_reply.started": "2024-09-11T11:27:57.149736Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directory: E:\\Upload AWS\\Extracted_Files_PDF\n",
      "Uploaded ,.jpg to S3 bucket mydaybucketlive in folder live/Image/.\n",
      "Uploaded 00bb070e107a99eb29259cad893a266.jpg to S3 bucket mydaybucketlive in folder live/Image/.\n",
      "Uploaded Application - Ruilong WANG - 108.pdf to S3 bucket mydaybucketlive in folder live/Image/.\n",
      "Uploaded Fwd Refund of deposit .pdf to S3 bucket mydaybucketlive in folder live/Image/.\n",
      "Upload complete. Data saved to E:\\Upload AWS\\Extracted_Files_PDF\\s3_uploaded_files_jpg.xlsx.\n"
     ]
    }
   ],
   "source": [
    "#jpg and pdf good\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# AWS Credentials\n",
    "AWS_ACCESS_KEY_ID = \"AKIAQ4WCASZZHAHYXDDA\"\n",
    "AWS_SECRET_ACCESS_KEY = \"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\"\n",
    "AWS_DEFAULT_REGION = \"ap-southeast-2\"\n",
    "AWS_BUCKET = \"mydaybucketlive\"\n",
    "\n",
    "# S3 folder name\n",
    "S3_FOLDER = \"live/Image/\"\n",
    "\n",
    "# Directory containing the .jpg and .pdf files\n",
    "directory_path = r\"E:\\Upload AWS\\Extracted_Files_PDF\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_DEFAULT_REGION\n",
    ")\n",
    "\n",
    "# List to store file names and their corresponding URLs\n",
    "data = []\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"Directory {directory_path} does not exist.\")\n",
    "else:\n",
    "    print(f\"Scanning directory: {directory_path}\")\n",
    "\n",
    "    # Upload files and generate object URLs\n",
    "    files_found = False\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Process only .jpg and .pdf files\n",
    "        if filename.endswith(\".jpg\") or filename.endswith(\".pdf\"):\n",
    "            files_found = True\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            try:\n",
    "                # Define S3 key (path) for the file inside the 'live/Image/' folder\n",
    "                s3_key = f\"{S3_FOLDER}{filename}\"\n",
    "\n",
    "                # Upload file to S3 under the 'live/Image/' folder\n",
    "                s3_client.upload_file(file_path, AWS_BUCKET, s3_key)\n",
    "                print(f\"Uploaded {filename} to S3 bucket {AWS_BUCKET} in folder {S3_FOLDER}.\")\n",
    "\n",
    "                # Generate the object URL\n",
    "                object_url = f\"https://{AWS_BUCKET}.s3.{AWS_DEFAULT_REGION}.amazonaws.com/{s3_key}\"\n",
    "\n",
    "                # Append file name and object URL to the data list\n",
    "                data.append({'File Name': filename, 'Object URL': object_url})\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {filename}: {str(e)}\")\n",
    "\n",
    "    if not files_found:\n",
    "        print(\"No .jpg or .pdf files found in the directory.\")\n",
    "\n",
    "    # If data is not empty, save to Excel\n",
    "    if data:\n",
    "        # Convert data to a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Define output Excel file path\n",
    "        output_excel = os.path.join(directory_path, 's3_uploaded_files_jpg.xlsx')\n",
    "\n",
    "        # Save to Excel\n",
    "        df.to_excel(output_excel, index=False)\n",
    "        print(f\"Upload complete. Data saved to {output_excel}.\")\n",
    "    else:\n",
    "        print(\"No files uploaded, Excel file was not generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4059a53-998f-45f3-b21c-6d93f6911f8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4627c2a4-7448-4588-b015-1cd3e5ed3bcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0cd455-afa4-4c0b-8505-1df1c1e273ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0da3b86-1777-4cb1-a4fe-a5ab7e404be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a13f5107-c4ed-4ebd-b8c3-cbec7cd8af08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T11:41:39.020873Z",
     "iopub.status.busy": "2024-09-11T11:41:39.020873Z",
     "iopub.status.idle": "2024-09-11T11:42:15.548505Z",
     "shell.execute_reply": "2024-09-11T11:42:15.548505Z",
     "shell.execute_reply.started": "2024-09-11T11:41:39.020873Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directory: E:\\Upload AWS\\Extracted_Files_PDF\n",
      "Uploaded Application - Ruilong WANG - 108.pdf to S3 bucket mydaybucketlive in folder live/Image/.\n",
      "Uploaded Fwd Refund of deposit .pdf to S3 bucket mydaybucketlive in folder live/Image/.\n",
      "Upload complete. Data saved to E:\\Upload AWS\\Extracted_Files_PDF\\s3_uploaded_files_pdf.xlsx.\n"
     ]
    }
   ],
   "source": [
    "#v2 final for pdf publicly acessible\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# AWS Credentials\n",
    "AWS_ACCESS_KEY_ID = \"AKIAQ4WCASZZHAHYXDDA\"\n",
    "AWS_SECRET_ACCESS_KEY = \"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\"\n",
    "AWS_DEFAULT_REGION = \"ap-southeast-2\"\n",
    "AWS_BUCKET = \"mydaybucketlive\"\n",
    "\n",
    "# S3 folder name\n",
    "S3_FOLDER = \"live/Image/\"\n",
    "\n",
    "# Directory containing the .pdf files\n",
    "directory_path = r\"E:\\Upload AWS\\Extracted_Files_PDF\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_DEFAULT_REGION\n",
    ")\n",
    "\n",
    "# List to store file names and their corresponding URLs\n",
    "data = []\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"Directory {directory_path} does not exist.\")\n",
    "else:\n",
    "    print(f\"Scanning directory: {directory_path}\")\n",
    "\n",
    "    # Upload files and generate object URLs\n",
    "    files_found = False\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Process only .pdf files\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            files_found = True\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            try:\n",
    "                # Define S3 key (path) for the file inside the 'live/PDFs/' folder\n",
    "                s3_key = f\"{S3_FOLDER}{filename}\"\n",
    "\n",
    "                # Upload file to S3 under the 'live/PDFs/' folder with correct Content-Type\n",
    "                s3_client.upload_file(\n",
    "                    file_path, \n",
    "                    AWS_BUCKET, \n",
    "                    s3_key,\n",
    "                    ExtraArgs={'ContentType': 'application/pdf'}\n",
    "                )\n",
    "                print(f\"Uploaded {filename} to S3 bucket {AWS_BUCKET} in folder {S3_FOLDER}.\")\n",
    "\n",
    "                # Generate the object URL\n",
    "                object_url = f\"https://{AWS_BUCKET}.s3.{AWS_DEFAULT_REGION}.amazonaws.com/{s3_key}\"\n",
    "\n",
    "                # Append file name and object URL to the data list\n",
    "                data.append({'File Name': filename, 'Object URL': object_url})\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {filename}: {str(e)}\")\n",
    "\n",
    "    if not files_found:\n",
    "        print(\"No .pdf files found in the directory.\")\n",
    "\n",
    "    # If data is not empty, save to Excel\n",
    "    if data:\n",
    "        # Convert data to a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Define output Excel file path\n",
    "        output_excel = os.path.join(directory_path, 's3_uploaded_files_pdf.xlsx')\n",
    "\n",
    "        # Save to Excel\n",
    "        df.to_excel(output_excel, index=False)\n",
    "        print(f\"Upload complete. Data saved to {output_excel}.\")\n",
    "    else:\n",
    "        print(\"No files uploaded, Excel file was not generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d94338-e80f-4f02-8804-bb682f0923d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9808b0-63b9-4887-a222-2ab518c6651d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3125250d-3361-4b42-bab2-2156ca0efafd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T11:12:47.959317Z",
     "iopub.status.busy": "2024-09-11T11:12:47.959317Z",
     "iopub.status.idle": "2024-09-11T11:14:34.287565Z",
     "shell.execute_reply": "2024-09-11T11:14:34.287453Z",
     "shell.execute_reply.started": "2024-09-11T11:12:47.959317Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directory: E:\\Upload AWS\\Extracted_Files_100PDF\n",
      "Uploaded Application - Ruilong WANG - 108.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded Fwd Refund of deposit .pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded Moment MA CAMP295.2(2).pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00014 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00016 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00018 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00026 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00045 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00066 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00068 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00076 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00082 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00090 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00093 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00098 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00107 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00122 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00135 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00142 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00148 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00155 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00167 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00168 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00183 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00186 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00199 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00209 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00240 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00246 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00250 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00254 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00257 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00263 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00269 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00274 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00279 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00288 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00289 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00294 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00302 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00305 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00314 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00327 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00334 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00337 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN00337 - Financial Summary 1 Jul 2022_1.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01039 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01049 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01153 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01166 - Financial Summary 1 Jul 2022.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01200 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01255 - Financial Summary 4 Jul 2023.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01414 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01523 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01572 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01616 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01647 - Financial Summary 4 Jul 2023.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01658 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01662 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01665 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01671 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01673 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01678 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01682 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01686 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01695 - Financial Summary 4 Jul 2023.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01710 - Financial Summary 4 Jul 2023.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01712 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01736 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01739 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01741 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN01746 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02084 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02123 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02188 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02302 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02330 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02333 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02333 - Financial Summary 1 Jul 2024_1.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02351 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02357 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02398 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02445 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02447 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02484 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02487 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02542 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02596 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02649 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02670 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02695 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02768 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded OWN02829 - Financial Summary 1 Jul 2024.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded Signed MA - ARNO17.1302.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded Statement #1 - OWN02799 .pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded Statement #1 - SAL02173.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Uploaded Statement #1 - SAL02847.pdf to S3 bucket mydaybucketlive in folder migration/.\n",
      "Upload complete. Data saved to E:\\Upload AWS\\Extracted_Files_100PDF\\s3_uploaded_files.xlsx.\n"
     ]
    }
   ],
   "source": [
    "#upload the .pdf files from the specified folder E:\\Upload AWS\\Extracted_Files_100PDF and generate the object URLs\n",
    "\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# AWS Credentials\n",
    "AWS_ACCESS_KEY_ID = \"AKIAQ4WCASZZHAHYXDDA\"\n",
    "AWS_SECRET_ACCESS_KEY = \"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\"\n",
    "AWS_DEFAULT_REGION = \"ap-southeast-2\"\n",
    "AWS_BUCKET = \"mydaybucketlive\"\n",
    "\n",
    "# S3 folder name\n",
    "S3_FOLDER = \"migration/\"\n",
    "\n",
    "# Directory containing the .pdf files\n",
    "directory_path = r\"E:\\Upload AWS\\Extracted_Files_100PDF\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_DEFAULT_REGION\n",
    ")\n",
    "\n",
    "# List to store file names and their corresponding URLs\n",
    "data = []\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"Directory {directory_path} does not exist.\")\n",
    "else:\n",
    "    print(f\"Scanning directory: {directory_path}\")\n",
    "\n",
    "    # Upload files and generate object URLs\n",
    "    files_found = False\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Process only .pdf files\n",
    "        if filename.endswith(\".pdf\"):\n",
    "            files_found = True\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            try:\n",
    "                # Define S3 key (path) for the file inside the 'live/PDFs/' folder\n",
    "                s3_key = f\"{S3_FOLDER}{filename}\"\n",
    "\n",
    "                # Upload file to S3 under the 'live/PDFs/' folder\n",
    "                s3_client.upload_file(file_path, AWS_BUCKET, s3_key)\n",
    "                print(f\"Uploaded {filename} to S3 bucket {AWS_BUCKET} in folder {S3_FOLDER}.\")\n",
    "\n",
    "                # Generate the object URL\n",
    "                object_url = f\"https://{AWS_BUCKET}.s3.{AWS_DEFAULT_REGION}.amazonaws.com/{s3_key}\"\n",
    "\n",
    "                # Append file name and object URL to the data list\n",
    "                data.append({'File Name': filename, 'Object URL': object_url})\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {filename}: {str(e)}\")\n",
    "\n",
    "    if not files_found:\n",
    "        print(\"No .pdf files found in the directory.\")\n",
    "\n",
    "    # If data is not empty, save to Excel\n",
    "    if data:\n",
    "        # Convert data to a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Define output Excel file path\n",
    "        output_excel = os.path.join(directory_path, 's3_uploaded_files.xlsx')\n",
    "\n",
    "        # Save to Excel\n",
    "        df.to_excel(output_excel, index=False)\n",
    "        print(f\"Upload complete. Data saved to {output_excel}.\")\n",
    "    else:\n",
    "        print(\"No files uploaded, Excel file was not generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa0fc61-9874-4197-af33-1d421cf89642",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e1328-6c23-47fe-ae3b-beabce5d8ab0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b86759-5a45-4fd0-9ac7-61ce45f46b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#v3 upload jpg files from folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9dcbc1-9509-4067-86e0-67921fcfa6b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea87c6-54b2-4376-abf0-c357c0dcee23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234c8261-0d73-4ac2-904c-10d342720d1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9512cff0-7614-4fda-b0ae-5f798feb82e2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-11T11:35:25.563004Z",
     "iopub.status.busy": "2024-09-11T11:35:25.563004Z",
     "iopub.status.idle": "2024-09-11T11:35:32.968997Z",
     "shell.execute_reply": "2024-09-11T11:35:32.968997Z",
     "shell.execute_reply.started": "2024-09-11T11:35:25.563004Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scanning directory: E:\\Upload AWS\\Extracted_Files_PDF\n",
      "Uploaded ,.jpg to S3 bucket mydaybucketlive in folder live/Image/.\n",
      "Uploaded 00bb070e107a99eb29259cad893a266.jpg to S3 bucket mydaybucketlive in folder live/Image/.\n",
      "Upload complete. Data saved to E:\\Upload AWS\\Extracted_Files_PDF\\s3_uploaded_files_jpg.xlsx.\n"
     ]
    }
   ],
   "source": [
    "#v3\n",
    "#image upload perfect code final\n",
    "\n",
    "\n",
    "import os\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# AWS Credentials\n",
    "AWS_ACCESS_KEY_ID = \"AKIAQ4WCASZZHAHYXDDA\"\n",
    "AWS_SECRET_ACCESS_KEY = \"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\"\n",
    "AWS_DEFAULT_REGION = \"ap-southeast-2\"\n",
    "AWS_BUCKET = \"mydaybucketlive\"\n",
    "\n",
    "# S3 folder name\n",
    "S3_FOLDER = \"live/Image/\"\n",
    "\n",
    "# Directory containing the .jpg files\n",
    "directory_path = r\"E:\\Upload AWS\\Extracted_Files_PDF\"\n",
    "\n",
    "# Initialize S3 client\n",
    "s3_client = boto3.client(\n",
    "    's3',\n",
    "    aws_access_key_id=AWS_ACCESS_KEY_ID,\n",
    "    aws_secret_access_key=AWS_SECRET_ACCESS_KEY,\n",
    "    region_name=AWS_DEFAULT_REGION\n",
    ")\n",
    "\n",
    "# List to store file names and their corresponding URLs\n",
    "data = []\n",
    "\n",
    "# Check if the directory exists\n",
    "if not os.path.exists(directory_path):\n",
    "    print(f\"Directory {directory_path} does not exist.\")\n",
    "else:\n",
    "    print(f\"Scanning directory: {directory_path}\")\n",
    "\n",
    "    # Upload files and generate object URLs\n",
    "    files_found = False\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Process only .jpg files\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            files_found = True\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            try:\n",
    "                # Define S3 key (path) for the file inside the 'live/Images/' folder\n",
    "                s3_key = f\"{S3_FOLDER}{filename}\"\n",
    "\n",
    "                # Upload file to S3 under the 'live/Images/' folder with correct Content-Type\n",
    "                s3_client.upload_file(\n",
    "                    file_path, \n",
    "                    AWS_BUCKET, \n",
    "                    s3_key,\n",
    "                    ExtraArgs={'ContentType': 'image/jpeg'}\n",
    "                )\n",
    "                print(f\"Uploaded {filename} to S3 bucket {AWS_BUCKET} in folder {S3_FOLDER}.\")\n",
    "\n",
    "                # Generate the object URL\n",
    "                object_url = f\"https://{AWS_BUCKET}.s3.{AWS_DEFAULT_REGION}.amazonaws.com/{s3_key}\"\n",
    "\n",
    "                # Append file name and object URL to the data list\n",
    "                data.append({'File Name': filename, 'Object URL': object_url})\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading {filename}: {str(e)}\")\n",
    "\n",
    "    if not files_found:\n",
    "        print(\"No .jpg files found in the directory.\")\n",
    "\n",
    "    # If data is not empty, save to Excel\n",
    "    if data:\n",
    "        # Convert data to a DataFrame\n",
    "        df = pd.DataFrame(data)\n",
    "\n",
    "        # Define output Excel file path\n",
    "        output_excel = os.path.join(directory_path, 's3_uploaded_files_jpg.xlsx')\n",
    "\n",
    "        # Save to Excel\n",
    "        df.to_excel(output_excel, index=False)\n",
    "        print(f\"Upload complete. Data saved to {output_excel}.\")\n",
    "    else:\n",
    "        print(\"No files uploaded, Excel file was not generated.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c00b796-1528-4355-8994-0279e87a5e7e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
